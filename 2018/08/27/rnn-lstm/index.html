<!DOCTYPE html><html><head><meta charset="utf-8"><title>Introduction to Sequence Models - RNN and LSTM | Franklin</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="description" content="Commercial TimeHey, what’s up, welcome to my third post. Recently I spent some time on Deep Learning study. I found an amazing Deep Learning Series (totally 5 specializations) by Andrew Ng on Coursera"><meta name="keywords" content="Deep Learning,Machine Learning,Study Notes"><meta property="og:type" content="article"><meta property="og:title" content="Introduction to Sequence Models - RNN and LSTM"><meta property="og:url" content="https://franklinwu19.github.io/2018/08/27/rnn-lstm/index.html"><meta property="og:site_name" content="Franklin"><meta property="og:description" content="Commercial TimeHey, what’s up, welcome to my third post. Recently I spent some time on Deep Learning study. I found an amazing Deep Learning Series (totally 5 specializations) by Andrew Ng on Coursera"><meta property="og:locale" content="default"><meta property="og:image" content="https://franklinwu19.github.io/2018/08/27/rnn-lstm/RNN.png"><meta property="og:image" content="https://franklinwu19.github.io/2018/08/27/rnn-lstm/rnn_step_forward.png"><meta property="og:image" content="https://franklinwu19.github.io/2018/08/27/rnn-lstm/rnn1.png"><meta property="og:image" content="https://franklinwu19.github.io/2018/08/27/rnn-lstm/LSTM.png"><meta property="og:image" content="https://franklinwu19.github.io/2018/08/27/rnn-lstm/LSTM_rnn.png"><meta property="og:updated_time" content="2018-08-28T14:36:13.989Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Introduction to Sequence Models - RNN and LSTM"><meta name="twitter:description" content="Commercial TimeHey, what’s up, welcome to my third post. Recently I spent some time on Deep Learning study. I found an amazing Deep Learning Series (totally 5 specializations) by Andrew Ng on Coursera"><meta name="twitter:image" content="https://franklinwu19.github.io/2018/08/27/rnn-lstm/RNN.png"><link rel="icon" href="/img/favicon.ico"><link href="https://fonts.googleapis.com/css?family=Lato:400,400i,700" rel="stylesheet"><link rel="stylesheet" href="/icomoon/style.css"><link rel="stylesheet" href="/style.css"></head><body><div class="site-wrapper"><div id="loading-bar-wrapper"><div id="loading-bar"></div></div><script>document.getElementById("loading-bar").style.width="20%"</script><header id="header" class="site-header clearfix"><a class="logo square clearfix" href="/"><span class="b">F </span><span class="w">r </span><span class="b">a </span><span class="b">n </span><span class="b">k </span><span class="w">l </span><span class="b">i </span><span class="w">n </span></a><a class="me square site-nav-switch clearfix"><span class="b"><span class="icon icon-menu"></span></span></a></header><script>document.getElementById("loading-bar").style.width="40%"</script><main id="main" class="clearfix"><article id="post-rnn-lstm" class="article white-box article-type-post" itemscope itemprop="blogPost"><header class="article-header"><h1 class="article-title" itemprop="name">Introduction to Sequence Models - RNN and LSTM</h1><div class="article-meta">Posted on <time class="article-time" datetime="2018-08-27T16:16:41.000Z" itemprop="datePublished">Aug 27, 2018</time></div></header><div class="article-entry" itemprop="articleBody"><h3 id="Commercial-Time"><a href="#Commercial-Time" class="headerlink" title="Commercial Time"></a>Commercial Time</h3><p>Hey, what’s up, welcome to my third post. Recently I spent some time on Deep Learning study. I found an amazing Deep Learning Series (totally 5 specializations) by <a href="http://www.andrewng.org/" target="_blank" rel="noopener">Andrew Ng</a> on <a href="https://www.coursera.org/" target="_blank" rel="noopener">Coursera</a>. I appended the <a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="noopener">Link</a> here.</p><p>The courses are developed by <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">Deeplearning.ai</a> in mid 2017, which has been almost 10 years since Andrew launched <a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Machine Learning</a> on Coursera (still <strong>THE BEST</strong> ML courses available online). New features and Python environment (with hands-on experience of how algorithm actually works and instructions to powerful libraries such as <code>Tensorflow</code> and <code>Keras</code>) are introduced, making it exciting to get started and break into AI/DL domain. (All you need is the basic knowledge of linear algebra, calculus and programming)</p><p>Especially, the <code>Jupyter Notebook</code> of this series are so beautiful and well-structured that I couldn’t help borrow lots of plots and text to make this post. If you are really interested, take them online now! XD</p><h3 id="Introduction-amp-Notations"><a href="#Introduction-amp-Notations" class="headerlink" title="Introduction &amp; Notations"></a>Introduction &amp; Notations</h3><p>Among various domain in deep learning, maybe sequence models are <strong>the most directly related</strong> to financial implementations. They are trying to deal with sequential data, which we could find everywhere in finance, e.g. financial news (NLP) or stock returns (time series). So sequence models provide us powerful tools to build a forecasting machine. In this post we will learn the basic of which, such as <strong>Recurrent Neural Network (RNN), Gated Recurrent Unit (GRU) Network </strong>and <strong>Long-Short Term Memory (LSTM) Network</strong>.</p><p>Unlike general neural network, where input are samples (data points) that not related to each other, the sequence models solve the task which <strong>requires combining knowledge within a group of samples, or a series of data points to make reasonable argument.</strong> For example, As human logic, intuitively the stock returns should be somehow related to the historical information, say, returns of a few days earlier. Let’s take a look!</p><p>Here, I assume you already know the basics of deep learning such as <strong>logistic regression</strong>, <strong>simple neural network</strong>, <strong>activation function</strong>, etc. While if you don’t, spending 30 minutes by googling the keywords and reading an introduction article will give you a roughly whole picture.</p><img src="/2018/08/27/rnn-lstm/RNN.png"><center><small><p style="color:gray"><b>Figure 1</b>: RNN network overview</p></small></center><h4 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h4><blockquote><ul><li><p>Superscript $[l]$ denotes an object associated with the $l^{th}$ layer.<br>Example: $a^{[4]}$ is the $4^{th}$ layer activation. $W^{[5]}$ and $b^{[5]}$ are the $5^{th}$ layer parameters.</p></li><li><p>Superscript $(i)$ denotes an object associated with the $i^{th}$ example.<br>Example: $x^{(i)}$ is the $i^{th}$ training example input.</p></li><li><p>Superscript $\langle t \rangle$ denotes an object at the $t^{th}$ time-step.<br>Example: $x^{\langle t \rangle}$ is the input x at the $t^{th}$ time-step. $x^{(i)\langle t \rangle}$ is the input at the $t^{th}$ timestep of example $i$.</p></li><li><p>Lowerscript $i$ denotes the $i^{th}$ entry of a vector.<br>Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the activations in layer $l$.</p></li></ul></blockquote><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>A Recurrent neural network can be seen as the repetition of a single cell (see $Figure\ 1$). Here we are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell.</p><img src="/2018/08/27/rnn-lstm/rnn_step_forward.png"><center><small><p style="color:gray"><b>Figure 2</b>: Basic RNN cell.</p></small></center><p>The above cell takes as input $x^{\langle t \rangle}$ (current input) and $a^{\langle t - 1\rangle}$ (previous hidden state containing information from the past), and outputs $a^{\langle t \rangle}$ which is given to the next RNN cell and also used to predict $y^{\langle t \rangle}$. Where <code>tanh</code> and <code>softmax</code> are two activation functions, which <code>tanh</code> is similar to <code>sigmoid</code> but more robust to usage scenarios, and <code>softmax</code> is a standard activation in classification problems.</p><p>You can see an RNN as the repetition of the cell you’ve just built. If your input sequence of data is carried over 10 time steps, then you will copy the RNN cell 10 times. Each cell takes as input the hidden state from the previous cell ($a^{\langle t-1 \rangle}$) and the current time-step’s input data ($x^{\langle t \rangle}$). It outputs a hidden state ($a^{\langle t \rangle}$) and a prediction ($y^{\langle t \rangle}$) for this time-step.</p><img src="/2018/08/27/rnn-lstm/rnn1.png"><center><small><p style="color:gray"><b>Figure 3</b>: Basic RNN. The input sequence $x = (x^{\langle 1 \rangle}, x^{\langle 2 \rangle}, ..., x^{\langle T_x \rangle})$ is carried over $T_x$ time steps. The network outputs $y = (y^{\langle 1 \rangle}, y^{\langle 2 \rangle}, ..., y^{\langle T_x \rangle})$.</p></small></center><p>The <strong>KEY</strong> that this type of neural network can solve the problem of <u><b>“How to avoid information inconsistence, i.e. the prediction may highly depends on the natural order (time series) of the data instead of given one input independently. “</b></u> For example, if we want to predict the next word of a sentence “他来自中国，他喜欢吃 ### “(“He comes from China, he loves(to eat) ###”).</p><p>As human, we read this sentence and extract information by sequence, first keyword: <strong>吃(eat)</strong> tells us that this word should be <strong>noun</strong>. ranther than <strong>verb. or adv.</strong>, and something related to eating, maybe a snack. And we look back to the second keyword <strong>中国(China)</strong>, which implies that this <strong>norn</strong> may related to Chinese food, so it is more reasonably to give an answer like <strong>火锅(Hot Pot)</strong> than <strong>电影(Movie)</strong> or <strong>跑步(Running)</strong>.</p><p>That’s exactly what we want machine to do. <strong><u>In RNN, given an input sequence, by adding extra information passed along entries by adding a term $a^{[t]}$ iterating neurons</u></strong>. While for the simple RNN, there is a problem called <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" target="_blank" rel="noopener">Gradient Vanishing or Explosion</a>, which refers to tuning parameter matrix on $a^{[t]}$ in backpropagation could make the matrix exponentially decays to zero along sequence. i.e. Roughly speaking, makes the further information vanishes (can’t remember memory that’s too long ago). To solve this, we need something more subtle by controlling information update in the architecture LTSM or GRU.</p><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>LSTM solves the problem mentioned above by adding gates to control the information update and what to pass into the next neuron. We post the Architecture of LSTM below:</p><img src="/2018/08/27/rnn-lstm/LSTM.png"><center><small><p style="color:gray"><b>Figure 4</b>: LSTM-cell. This tracks and updates a "cell state" or memory variable $c^{\langle t \rangle}$ at every time-step, which can be different from $a^{\langle t \rangle}$.</p></small></center><h4 id="About-the-Gates"><a href="#About-the-Gates" class="headerlink" title="About the Gates"></a>About the Gates</h4><blockquote><ul><li><p><strong>Forget Gate</strong></p><p>For the sake of this illustration, lets assume we are reading words in a piece of text, and want use an LSTM to keep track of grammatical structures, such as whether the subject is singular or plural. If the subject changes from a singular word to a plural word, we need to find a way to get rid of our previously stored memory value of the singular/plural state. In an LSTM, the forget gate lets us do this:</p></li></ul><script type="math/tex;mode=display">\Gamma_f^{\langle t \rangle} = \sigma(W_f[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_f)\tag{1}</script><p style="padding:2em">Here, $W_f$ are weights that govern the forget gate's behavior. We concatenate $[a^{\langle t-1 \rangle}, x^{\langle t \rangle}]$ and multiply by $W_f$. The equation above results in a vector $\Gamma_f^{\langle t \rangle}$ with values between 0 and 1. This forget gate vector will be multiplied element-wise by the previous cell state $c^{\langle t-1 \rangle}$. So if one of the values of $\Gamma_f^{\langle t \rangle}$ is 0 (or close to 0) then it means that the LSTM should remove that piece of information (e.g. the singular subject) in the corresponding component of $c^{\langle t-1 \rangle}$. If one of the values is 1, then it will keep the information.</p><ul><li><p><strong>Update Gate</strong></p><p>Once we forget that the subject being discussed is singular, we need to find a way to update it to reflect that the new subject is now plural. Here is the formulat for the update gate:</p></li></ul><script type="math/tex;mode=display">\Gamma_u^{\langle t \rangle} = \sigma(W_u[a^{\langle t-1 \rangle}, x^{\{t\}}] + b_u)\tag{2}</script><p style="padding:2em">Similar to the forget gate, here $\Gamma_u^{\langle t \rangle}$ is again a vector of values between 0 and 1. This will be multiplied element-wise with $\tilde{c}^{\langle t \rangle}$, in order to compute $c^{\langle t \rangle}$.</p><ul><li><strong>Updating the Cell</strong><br>To update the new subject we need to create a new vector of numbers that we can add to our previous cell state. The equation we use is: &lt;/p&gt;<script type="math/tex;mode=display">\tilde{c}^{\langle t \rangle} = \tanh(W_c[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_c)\tag{3}</script></li></ul><p style="padding:2em">Finally, the new cell state is:</p><script type="math/tex;mode=display">c^{\langle t \rangle} = \Gamma_f^{\langle t \rangle}* c^{\langle t-1 \rangle} + \Gamma_u^{\langle t \rangle} * \tilde{c}^{\langle t \rangle} \tag{4}</script><ul><li><p><strong>Output Gate</strong></p><p>To decide which outputs we will use, we will use the following two formulas:</p></li></ul><script type="math/tex;mode=display">\Gamma_o^{\langle t \rangle}=  \sigma(W_o[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_o)\tag{5}</script><script type="math/tex;mode=display">a^{\langle t \rangle} = \Gamma_o^{\langle t \rangle}* \tanh(c^{\langle t \rangle})\tag{6}</script><p style="padding:2em">Where in equation 5 you decide what to output using a sigmoid function and in equation 6 you multiply that by the $\tanh$ of the previous state.</p></blockquote><p>Now that you have implemented one step of an LSTM, you can now iterate this over this using a for-loop to process a sequence of $T_x$ inputs.</p><img src="/2018/08/27/rnn-lstm/LSTM_rnn.png"><center><small><p style="color:gray"><b>Figure 5</b>: LSTM over multiple time-steps.</p></small></center><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>Now we have come through RNN and LSTM, hopefully this post will prepare you the basic knowledge and intuition about how sequence model works. Using <code>Keras</code> library, we can simply build a deep learning models easily, <code>numpy</code> library also provides building-blocks for deep learning e.g. <code>broadcasting</code> to make Tensor Operation ( polynomial computation) hundreds of times faster than for-loop.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line"><span class="comment"># code snippet of constructing a simple network</span></span><br><span class="line"><span class="comment"># taking 10000 input points, with 3 hidden layers</span></span><br><span class="line"><span class="comment"># making binary classification</span></span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">2</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'catgorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># train model!</span></span><br><span class="line">model.fit(x_train,</span><br><span class="line">          y_train,</span><br><span class="line">          epocgs=<span class="number">10</span>,</span><br><span class="line">          batch_size=<span class="number">512</span>,</span><br><span class="line">          validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure><p>In the following posts, I plan to add some basic Deep Learning knowledge (logistic regression, etc.), Environment Setup (Ubuntu, GPU, AWS cloud, etc.) and our <strong>final goal is to building a financial deep learning forecaster to construct trading strategies.</strong></p><p>There are lots of resources that you can get access to online. I will recommend a book here: <a href="https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438" target="_blank" rel="noopener">Deep Learning with Python</a>, an amazing hands-on instructions for <code>Keras</code> library, you can’t even find a single math formula in Latex! since everything is written in programming language. (interpreted in loop) Despite the book, I would still suggest taking formal courses in order to better understand how deep learning works in <strong>algebra language.</strong> Also <a href="http://colah.github.io/" target="_blank" rel="noopener">colah’s blog</a> is a good place to learn, his posts were officially mentioned by Andrew Ng, especially this <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a>, truly beautiful.</p><p>If you like this post, share it with your friends! XD</p></div><div class="article-tags"><a class="tag-link" href="/tags/Deep-Learning/">Deep Learning</a><a class="tag-link" href="/tags/Machine-Learning/">Machine Learning</a><a class="tag-link" href="/tags/Study-Notes/">Study Notes</a></div><section id="comments" class="white-box"><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div></section></article><script>document.getElementById("loading-bar").style.width="60%"</script></main><script>var disqus_shortname="franklin-me",disqus_url="https://franklinwu19.github.io/about/index.html";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//go.disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}()</script><div id="disqus_thread"></div><footer id="footer" class="clearfix"><div class="search"><form name="searchform" id="searchform" class="u-search-form"><input type="text" id="searchinput" class="u-search-input st-default-search-input" data-list-highlight="true" data-list-value-completion="true" placeholder="Looking for something?"> <button type="submit" id="u-search-btn-submit" class="u-search-btn-submit"><span class="icon icon-search"></span></button></form></div><div>&copy; <a href="https://franklinwu19.github.io">Franklin</a> Theme by <a href="http://artifact.me/" target="_blank">Art Chen</a>.</div><div>Powered by <a href="https://hexo.io/" rel="external">Hexo</a>.</div></footer><script>document.getElementById("loading-bar").style.width="80%"</script><div class="overlay"></div></div><div class="site-sidebar"><div class="sidebar-switch clearfix" style="display:none"><a class="dark-btn active" data-toggle="toc"><span class="icon icon-list"></span> <span class="text">Index</span> </a><a class="dark-btn" data-toggle="bio"><span class="icon icon-person"></span> <span class="text">Bio</span></a></div><div class="site-toc" style="display:none"><div class="no-index">No Index</div></div><div class="site-bio show" style="display:block"><div class="about-me clearfix"><div class="avatar"><img src="/img/avatar.png"></div><div class="info"><a class="name dark-btn" href="/about">Franklin Wu</a></div><div class="info"><span class="item desc"></span></div></div><div class="social clearfix"><a href="/atom.xml" class="feed" target="_blank" rel="external"><span class="icon icon-feed"></span> </a><a href="https://github.com/franklinwu19" class="github" target="_blank" rel="external"><span class="icon icon-github"></span> </a><a href="https://twitter.com/Wu_Zhunghao" class="twitter" target="_blank" rel="external"><span class="icon icon-twitter"></span> </a><a href="https://www.linkedin.com/in/franklinng/" class="linkedin" target="_blank" rel="external"><span class="icon icon-linkedin"></span></a></div><div class="shortcuts clearfix"><div class="bk"><a href="#header" class="dark-btn window-nav"><span class="icon icon-chevron-thin-up"></span> <span class="text">Back to Top</span></a></div><div class="bk"><a href="#footer" class="dark-btn window-nav"><span class="icon icon-chevron-thin-down"></span> <span class="text">Go to Bottom</span></a></div></div></div></div><script type="text/javascript">var GOOGLE_CUSTOM_SEARCH_API_KEY="",GOOGLE_CUSTOM_SEARCH_ENGINE_ID="",ALGOLIA_API_KEY="",ALGOLIA_APP_ID="",ALGOLIA_INDEX_NAME="",AZURE_SERVICE_NAME="",AZURE_INDEX_NAME="",AZURE_QUERY_KEY="",BAIDU_API_ID="",SEARCH_SERVICE="hexo"</script><script src="https://code.jquery.com/jquery-2.1.4.min.js"></script><script>window.jQuery||document.write('<script src="/js/jquery.js"><\/script>')</script><script src="/js/search.js"></script><script src="/js/app.js"></script><script type="text/javascript">var disqus_shortname="franklin-me",disqus_url="https://franklinwu19.github.io/2018/08/27/rnn-lstm/";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}()</script><script>document.getElementById("loading-bar").style.width="100%"</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>